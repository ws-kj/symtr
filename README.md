# Teaching Compact Transformers to Plan Through Domain Anonymization

## Overview

The goal of this project is to distill generalized symbolic planning capabilities into a ~1B parameter transformer model, suitable for fast inference on compute-constrained devices.

## Motivation

Previous attempts to imitate planner output with transformer models have performed well on in-distribution problems, but fail to generalize outside of the domain that they fine-tuned on. This project will attempt to overcome the generalization barrier by anonymizing domain formalisms, forcing the model to learn planning as an abstract skill.

## Methodology

### Dataset Collection
First, a large and diverse dataset of PDDL plans generated by a classical planner (such as FastDownward) will be collected. 

### Domain Anonymization
To encourage generalization, these plans will be **anonymized** by converting all domain-specific identifiers (`moveBlock`, `robot`, `onTable`, `location1`, etc.) into a standard set (`action1`, `action2`, `predicate1`, etc.) shared by all domains.

### Training Pipeline
1. **Supervised Fine-Tuning (SFT)** will be performed on a <2B parameter transformer model using the anonymized domains and solved plans
2. **Post-training optimization** will be done with GRPO or equivalent to reward the generation of optimal and compact plans

## Relevant Literature

- [Planning with Large Language Models via Search and Imitation Learning](https://arxiv.org/html/2404.03683v1)
- [Can Foundation Models Plan? A Study on Planning with Large Language Models](https://arxiv.org/html/2303.00438v3)
- [Learning Domain-Independent Planning Heuristics with Hypergraph Networks](https://arxiv.org/pdf/2212.08681)

## Resources

- [Pyperplan PDDL Benchmarks](https://github.com/aibasel/pyperplan/tree/main/benchmarks)
