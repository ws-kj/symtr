# Teaching compact transformers to plan through domain anonymization

The goal of this project is to distill generalized symbolic planning capabilities into a ~1B parameter transformer model, suitable for fast inference on compute-constrained devices.

Previous attempts to imitate planner output with transformer models have performed well on in-distribution problems, but fail to generalize outside of the domain that they fine-tuned on. This project will attempt to overcome the generalization barrier by anonymizing domain formalisms, forcing the model to learn planning as an abstract skill.

First, a large and diverse dataset of PDDL plans generated by a classical planner (such as FastDownward) will be collected. To encourage generalization, these plans will be *anonymized* by converting all domain-specific identifiers ("moveBlock", "robot", "onTable", "location1", etc.) into a standard set ("action1", "action2", "predicate1", etc.) shared by all domains. 
SFT will then be performed on a <2B parameter transformer model using the anonymized domains and solved plans.
A final post-training step will then be doine with GRPO or equivalent to reward the generation of optimal and compact plans.


Relevant literature/links:
https://arxiv.org/html/2404.03683v1
https://arxiv.org/html/2303.00438v3
https://arxiv.org/pdf/2212.08681
https://github.com/aibasel/pyperplan/tree/main/benchmarks/logistics
